{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "chatbot-challenge-nlp-dl-bAbI.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JOkE0wwga5tN"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0D_p9Oza5o6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Question and Answer Chat Bots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Jw8AAja5o-",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "We will be working with the Babi Data Set from Facebook Research.\n",
        "\n",
        "Full Details: https://research.fb.com/downloads/babi/\n",
        "\n",
        "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
        "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
        "  http://arxiv.org/abs/1502.05698\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AaGJVfza5pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-9KW2txa5pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"train_qa.txt\", \"rb\") as fp:   # Unpickling\n",
        "    train_data =  pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ2vuF3ua5pM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"test_qa.txt\", \"rb\") as fp:   # Unpickling\n",
        "    test_data =  pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7iuIL3Ha5pR",
        "colab_type": "text"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VohzJNAia5pT",
        "colab_type": "text"
      },
      "source": [
        "### Tip: It may be a good idea to explore the dataset!\n",
        "\n",
        "Below is just a sample of what you can do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3BlD0zga5pV",
        "colab_type": "code",
        "outputId": "06e3d8ed-0580-45f5-d6b4-7857ccfe96e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(test_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_PWJptea5pn",
        "colab_type": "code",
        "outputId": "6519c20e-3742-4ff7-d0da-be6da525319d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfflSXlNa5ps",
        "colab_type": "code",
        "outputId": "62f4834b-d0e2-4ebb-e2bb-5a5749b1a06e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Mary',\n",
              "  'moved',\n",
              "  'to',\n",
              "  'the',\n",
              "  'bathroom',\n",
              "  '.',\n",
              "  'Sandra',\n",
              "  'journeyed',\n",
              "  'to',\n",
              "  'the',\n",
              "  'bedroom',\n",
              "  '.'],\n",
              " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
              " 'no')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR7lT-UNa5qD",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "## Setting up Vocabulary of All Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPHxf3JXa5qE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a set that holds the vocab words\n",
        "vocab = set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yroUGLs7a5qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = test_data + train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSnJXuz1a5qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for story, question , answer in all_data:\n",
        "    # In case you don't know what a union of sets is:\n",
        "    # https://www.programiz.com/python-programming/methods/set/union\n",
        "    vocab = vocab.union(set(story))\n",
        "    vocab = vocab.union(set(question))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0V4ALeAa5qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Include any other words in the bot's vocabulary\n",
        "vocab.add('no')\n",
        "vocab.add('yes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X37F4gEZa5qa",
        "colab_type": "code",
        "outputId": "9888c647-4b40-47ad-8709-4bb713422276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " '?',\n",
              " 'Daniel',\n",
              " 'Is',\n",
              " 'John',\n",
              " 'Mary',\n",
              " 'Sandra',\n",
              " 'apple',\n",
              " 'back',\n",
              " 'bathroom',\n",
              " 'bedroom',\n",
              " 'discarded',\n",
              " 'down',\n",
              " 'dropped',\n",
              " 'football',\n",
              " 'garden',\n",
              " 'got',\n",
              " 'grabbed',\n",
              " 'hallway',\n",
              " 'in',\n",
              " 'journeyed',\n",
              " 'kitchen',\n",
              " 'left',\n",
              " 'milk',\n",
              " 'moved',\n",
              " 'no',\n",
              " 'office',\n",
              " 'picked',\n",
              " 'put',\n",
              " 'the',\n",
              " 'there',\n",
              " 'to',\n",
              " 'took',\n",
              " 'travelled',\n",
              " 'up',\n",
              " 'went',\n",
              " 'yes'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmUOXfRva5qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAH0SnYYa5qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_story_len = max([len(data[0]) for data in all_data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fbcSZGQa5qp",
        "colab_type": "code",
        "outputId": "c0e30c04-b7fb-4344-8598-ddf19807590b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_story_len"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsZFkb7ra5q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_question_len = max([len(data[1]) for data in all_data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLD1Pyl4a5q7",
        "colab_type": "code",
        "outputId": "20e2397a-08b2-411d-ab77-9c4afddf9e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_question_len"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZlHWiyda5q_",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38vgdT3Ma5rA",
        "colab_type": "code",
        "outputId": "6d32e32c-2bac-49ff-9f99-5f7c214d159b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " '?',\n",
              " 'Daniel',\n",
              " 'Is',\n",
              " 'John',\n",
              " 'Mary',\n",
              " 'Sandra',\n",
              " 'apple',\n",
              " 'back',\n",
              " 'bathroom',\n",
              " 'bedroom',\n",
              " 'discarded',\n",
              " 'down',\n",
              " 'dropped',\n",
              " 'football',\n",
              " 'garden',\n",
              " 'got',\n",
              " 'grabbed',\n",
              " 'hallway',\n",
              " 'in',\n",
              " 'journeyed',\n",
              " 'kitchen',\n",
              " 'left',\n",
              " 'milk',\n",
              " 'moved',\n",
              " 'no',\n",
              " 'office',\n",
              " 'picked',\n",
              " 'put',\n",
              " 'the',\n",
              " 'there',\n",
              " 'to',\n",
              " 'took',\n",
              " 'travelled',\n",
              " 'up',\n",
              " 'went',\n",
              " 'yes'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VASh8nIa5rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reserve 0 for pad_sequences\n",
        "vocab_size = len(vocab) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-if2LNba5rM",
        "colab_type": "text"
      },
      "source": [
        "-----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ4IS6a6a5rN",
        "colab_type": "code",
        "outputId": "8c0db174-1b23-47c4-d4d0-82a33426c6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B1PWhoFa5rQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer(filters=[])\n",
        "\n",
        "# TODO: Fit tokenizer on text\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83AGtmI4a5rT",
        "colab_type": "code",
        "outputId": "2d4b1375-a8e4-4c30-94ad-96762f12290d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 37,\n",
              " '?': 7,\n",
              " 'apple': 27,\n",
              " 'back': 3,\n",
              " 'bathroom': 28,\n",
              " 'bedroom': 31,\n",
              " 'daniel': 6,\n",
              " 'discarded': 15,\n",
              " 'down': 10,\n",
              " 'dropped': 4,\n",
              " 'football': 34,\n",
              " 'garden': 9,\n",
              " 'got': 12,\n",
              " 'grabbed': 1,\n",
              " 'hallway': 23,\n",
              " 'in': 18,\n",
              " 'is': 8,\n",
              " 'john': 33,\n",
              " 'journeyed': 25,\n",
              " 'kitchen': 19,\n",
              " 'left': 13,\n",
              " 'mary': 21,\n",
              " 'milk': 35,\n",
              " 'moved': 32,\n",
              " 'no': 2,\n",
              " 'office': 26,\n",
              " 'picked': 30,\n",
              " 'put': 20,\n",
              " 'sandra': 36,\n",
              " 'the': 11,\n",
              " 'there': 16,\n",
              " 'to': 5,\n",
              " 'took': 29,\n",
              " 'travelled': 14,\n",
              " 'up': 24,\n",
              " 'went': 22,\n",
              " 'yes': 17}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc0-V6aMa5rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_story_text = []\n",
        "train_question_text = []\n",
        "train_answers = []\n",
        "\n",
        "# TODO: Fill the story, question, and answers list\n",
        "for story,question,answer in train_data:\n",
        "    train_story_text.append(story)\n",
        "    train_question_text.append(question) \n",
        "    train_answers.append(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pse59hvwa5rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Vectorize into word sequences.\n",
        "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdPIW9Fja5rk",
        "colab_type": "code",
        "outputId": "789d1ccf-2c66-468b-87e5-c60dee78c69b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_story_text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hc0L1-Ja5ru",
        "colab_type": "code",
        "outputId": "dace3848-3b4c-43bc-e56c-61ae58b45ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_story_seq)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3GCN3cva5sC",
        "colab_type": "text"
      },
      "source": [
        "### Functionalize Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIm6vjlca5sE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_stories(data, \n",
        "                      word_index=tokenizer.word_index, \n",
        "                      max_story_len=max_story_len,\n",
        "                      max_question_len=max_question_len):\n",
        "    '''\n",
        "    INPUT: \n",
        "    \n",
        "    data: consisting of Stories,Queries,and Answers\n",
        "    word_index: word index dictionary from tokenizer\n",
        "    max_story_len: the length of the longest story (used for pad_sequences function)\n",
        "    max_question_len: length of the longest question (used for pad_sequences function)\n",
        "\n",
        "\n",
        "    OUTPUT:\n",
        "    \n",
        "    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and\n",
        "    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate\n",
        "    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
        "    \n",
        "    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    # X = STORIES\n",
        "    X = []\n",
        "    # Xq = QUERY/QUESTION\n",
        "    Xq = []\n",
        "    # Y = CORRECT ANSWER\n",
        "    Y = []\n",
        "    \n",
        "    \n",
        "    for story, query, answer in data:\n",
        "        x = []\n",
        "        xq = []\n",
        "        y = []\n",
        "\n",
        "        # TODO: Store every word from story into a list\n",
        "        # TODO: Store every word from query into a list\n",
        "        \n",
        "        # TODO: One-hot encode the label into a list\n",
        "        \n",
        "        x = [word_index[word.lower()] for word in story]\n",
        "        \n",
        "        xq = [word_index[word.lower()] for word in question]\n",
        "        \n",
        "        y = np.zeros(len(word_index) + 1) #Index 0 Reserved when padding the sequences\n",
        "        y[word_index[answer]] = 1\n",
        "        # Append each set of story,query, and answer to their respective holding lists\n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "        Y.append(y)\n",
        "        \n",
        "    # RETURN TUPLE of paded, uniform sequences FOR UNPACKING\n",
        "    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iFoz6nVa5sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eejFwvhua5sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGRoTxura5sV",
        "colab_type": "code",
        "outputId": "a9c374bd-8ede-4bb0-82a3-872e3eb9b9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "inputs_test"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ..., 11, 31, 37],\n",
              "       [ 0,  0,  0, ..., 11,  9, 37],\n",
              "       [ 0,  0,  0, ..., 11,  9, 37],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 11, 27, 37],\n",
              "       [ 0,  0,  0, ..., 11,  9, 37],\n",
              "       [ 0,  0,  0, ..., 27, 16, 37]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ-_qK08a5sf",
        "colab_type": "code",
        "outputId": "d522565f-2981-476a-9324-8babc0d434d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "queries_test"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8, 21, 18, 11, 31,  7],\n",
              "       [ 8, 21, 18, 11, 31,  7],\n",
              "       [ 8, 21, 18, 11, 31,  7],\n",
              "       ...,\n",
              "       [ 8, 21, 18, 11, 31,  7],\n",
              "       [ 8, 21, 18, 11, 31,  7],\n",
              "       [ 8, 21, 18, 11, 31,  7]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca9q6kpVa5sj",
        "colab_type": "code",
        "outputId": "9e2cb693-d654-4227-b911-9cd8bbe80887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "answers_test"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0Zzvr-Ba5sm",
        "colab_type": "code",
        "outputId": "1f822c71-6d2c-46d2-d0a0-4f66af1788b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "sum(answers_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.,   0., 503.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0., 497.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cf-rfn1a5st",
        "colab_type": "code",
        "outputId": "61d3a434-e2be-43de-d822-c70cb0d30186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.word_index['yes']"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bediffima5sy",
        "colab_type": "code",
        "outputId": "77d30146-c4e0-44ac-edcc-6f8b9296d875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.word_index['no']"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu6Xo-Noa5s5",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFESvEZsa5s8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
        "from keras.layers import add, dot, concatenate\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-YOF6oDa5tG",
        "colab_type": "text"
      },
      "source": [
        "### Placeholders for Inputs\n",
        "\n",
        "Recall we technically have two inputs, stories and questions. So we need to use placeholders. `Input()` is used to instantiate a Keras tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqCPMwsla5tH",
        "colab_type": "code",
        "outputId": "c8fcdc6b-1ae3-4a36-815d-e722b3619b01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "input_sequence = Input((max_story_len,))\n",
        "question = Input((max_question_len,))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "JOkE0wwga5tN",
        "colab_type": "text"
      },
      "source": [
        "### Building the Networks\n",
        "\n",
        "To understand why we chose this setup, make sure to read the paper we are using:\n",
        "\n",
        "* Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
        "  \"End-To-End Memory Networks\",\n",
        "  http://arxiv.org/abs/1503.08895"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NJeRFIta5tP",
        "colab_type": "text"
      },
      "source": [
        "## Encoders\n",
        "\n",
        "The input to your neural network for an NLP task requires you to setup an Embedding layer which creates word embedding for you(aka word2vec).\n",
        "\n",
        "Also it would be a good idea to experiment with different hyperparameters like using/not using dropout layers, learning rate etc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_7fsR9rMFY0",
        "colab_type": "text"
      },
      "source": [
        "### Input Encoder m"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIUWSinPa5tR",
        "colab_type": "code",
        "outputId": "bba19e66-864a-4b41-e66a-1d0175ba8282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Input gets embedded to a sequence of vectors\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\n",
        "\n",
        "# Optional: Create any additional layers for neural network"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQEqTIala5tW",
        "colab_type": "text"
      },
      "source": [
        "### Input Encoder c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF3erOIXa5tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embed the input into a sequence of vectors of size query_maxlen\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\n",
        "\n",
        "# Optional: Create any additional layers for neural network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii7TEQLHa5ti",
        "colab_type": "text"
      },
      "source": [
        "### Question Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt9kKE3La5tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embed the question into a sequence of vectors\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size,\n",
        "                               output_dim=64,\n",
        "                               input_length=max_question_len))\n",
        "\n",
        "# Optional: Create any additional layers for neural network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_I8ywjDa5to",
        "colab_type": "text"
      },
      "source": [
        "### Encode the Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap2fFowPa5tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Encode input sequence and questions (which are indices)\n",
        "# to sequences of dense vectors\n",
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgN7i1aa5tv",
        "colab_type": "text"
      },
      "source": [
        "##### Use dot product to compute the match between first input vector seq and the query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_W9OTBca5ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shape: `(samples, story_maxlen, query_maxlen)`\n",
        "match = dot([input_encoded_m,question_encoded], axes = (2,2))\n",
        "match = Activation('softmax')(match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnQcWYIja5t2",
        "colab_type": "text"
      },
      "source": [
        "#### Add this match matrix with the second input vector sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq6d15Yba5t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add the match matrix with the second input vector sequence\n",
        "response = add([match,input_encoded_c])\n",
        "response = Permute((2,1))(response) #Permute Layer: permutes dimensions of input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqv0cWNBa5t9",
        "colab_type": "text"
      },
      "source": [
        "#### Concatenate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TfL3iy1a5t_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate the match matrix with the question vector sequence\n",
        "answer = concatenate([response, question_encoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klww8nV0a5uG",
        "colab_type": "code",
        "outputId": "61dfeabe-7062-4d98-a7c8-b3bb4fd3963e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "answer"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'concatenate_1/concat:0' shape=(?, 6, 220) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG7kVtSda5uJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer = LSTM(32)(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_B3i2n_a5uO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "78750081-3c8b-4022-a963-f724c191d953"
      },
      "source": [
        "# Regularization with Dropout\n",
        "answer = Dropout(0.5)(answer)\n",
        "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TboZRHW3a5uU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "82908931-decd-4e4f-efb5-11d4370d4138"
      },
      "source": [
        "# TODO :Output a probability distribution over the vocabulary\n",
        "answer = Activation('softmax')(answer)\n",
        "#Build the model\n",
        "model = Model([input_sequence,question], answer)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UywAOEZsa5uW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "cfbf2b62-9e77-4982-dab8-3c1945fafcbb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 156)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 6)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       multiple             2432        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 6, 64)        2432        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 156, 6)       0           sequential_1[1][0]               \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 156, 6)       0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       multiple             228         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 156, 6)       0           activation_1[0][0]               \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 6, 156)       0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 6, 220)       0           permute_1[0][0]                  \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 32)           32384       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 38)           1254        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 38)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 38,730\n",
            "Trainable params: 38,730\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrpXLZFZa5ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39372657-ab84-4f63-da46-bc7f41036e12"
      },
      "source": [
        "# TODO: Train Model\n",
        "epochs = 100\n",
        "batch_size=32\n",
        "history = model.fit([inputs_train,queries_train],answers_train, batch_size = batch_size, epochs = epochs, validation_data = ([inputs_test,queries_test],answers_test))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "10000/10000 [==============================] - 7s 653us/step - loss: 0.8979 - acc: 0.4948 - val_loss: 0.6945 - val_acc: 0.5030\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 5s 495us/step - loss: 0.7049 - acc: 0.4958 - val_loss: 0.6943 - val_acc: 0.5030\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 5s 488us/step - loss: 0.6950 - acc: 0.5109 - val_loss: 0.6935 - val_acc: 0.5030\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 5s 509us/step - loss: 0.6955 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4930\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 5s 486us/step - loss: 0.6948 - acc: 0.5003 - val_loss: 0.6937 - val_acc: 0.4970\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 5s 482us/step - loss: 0.6944 - acc: 0.5062 - val_loss: 0.6932 - val_acc: 0.4970\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 5s 498us/step - loss: 0.6946 - acc: 0.5060 - val_loss: 0.6932 - val_acc: 0.5030\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 5s 517us/step - loss: 0.6946 - acc: 0.4927 - val_loss: 0.6931 - val_acc: 0.5030\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 5s 488us/step - loss: 0.6946 - acc: 0.4935 - val_loss: 0.6932 - val_acc: 0.4970\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 0.6942 - acc: 0.5039 - val_loss: 0.6960 - val_acc: 0.5030\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 0.6938 - acc: 0.5047 - val_loss: 0.6934 - val_acc: 0.5030\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 5s 492us/step - loss: 0.6945 - acc: 0.4975 - val_loss: 0.6966 - val_acc: 0.5030\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 0.6941 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.4970\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 0.6943 - acc: 0.4938 - val_loss: 0.6942 - val_acc: 0.4970\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 0.6936 - acc: 0.5063 - val_loss: 0.6933 - val_acc: 0.5050\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 5s 511us/step - loss: 0.6937 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.5130\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 5s 488us/step - loss: 0.6925 - acc: 0.5204 - val_loss: 0.6940 - val_acc: 0.5050\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 0.6919 - acc: 0.5265 - val_loss: 0.6947 - val_acc: 0.5210\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 5s 495us/step - loss: 0.6907 - acc: 0.5338 - val_loss: 0.6973 - val_acc: 0.5050\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 5s 527us/step - loss: 0.6896 - acc: 0.5317 - val_loss: 0.6976 - val_acc: 0.4970\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 5s 501us/step - loss: 0.6886 - acc: 0.5384 - val_loss: 0.6989 - val_acc: 0.4960\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 5s 484us/step - loss: 0.6878 - acc: 0.5413 - val_loss: 0.7023 - val_acc: 0.4970\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 5s 499us/step - loss: 0.6872 - acc: 0.5430 - val_loss: 0.6992 - val_acc: 0.5050\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 5s 468us/step - loss: 0.6860 - acc: 0.5451 - val_loss: 0.7072 - val_acc: 0.4800\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 5s 486us/step - loss: 0.6848 - acc: 0.5468 - val_loss: 0.7026 - val_acc: 0.4840\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.6835 - acc: 0.5509 - val_loss: 0.7048 - val_acc: 0.4930\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 5s 520us/step - loss: 0.6818 - acc: 0.5501 - val_loss: 0.7051 - val_acc: 0.5010\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 5s 526us/step - loss: 0.6793 - acc: 0.5564 - val_loss: 0.7053 - val_acc: 0.5000\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 5s 499us/step - loss: 0.6791 - acc: 0.5542 - val_loss: 0.7042 - val_acc: 0.4960\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 5s 492us/step - loss: 0.6763 - acc: 0.5610 - val_loss: 0.7067 - val_acc: 0.4910\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 5s 494us/step - loss: 0.6721 - acc: 0.5710 - val_loss: 0.7079 - val_acc: 0.5030\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 5s 501us/step - loss: 0.6693 - acc: 0.5759 - val_loss: 0.7072 - val_acc: 0.4960\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 0.6664 - acc: 0.5793 - val_loss: 0.7106 - val_acc: 0.5040\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 5s 490us/step - loss: 0.6616 - acc: 0.5897 - val_loss: 0.7205 - val_acc: 0.5110\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 5s 495us/step - loss: 0.6547 - acc: 0.5937 - val_loss: 0.7125 - val_acc: 0.5050\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 0.6520 - acc: 0.6001 - val_loss: 0.7185 - val_acc: 0.4850\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 0.6468 - acc: 0.6025 - val_loss: 0.7158 - val_acc: 0.5100\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 5s 480us/step - loss: 0.6436 - acc: 0.6087 - val_loss: 0.7184 - val_acc: 0.5030\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 5s 507us/step - loss: 0.6360 - acc: 0.6189 - val_loss: 0.7254 - val_acc: 0.5060\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 5s 474us/step - loss: 0.6300 - acc: 0.6196 - val_loss: 0.7403 - val_acc: 0.5070\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 5s 507us/step - loss: 0.6263 - acc: 0.6272 - val_loss: 0.7396 - val_acc: 0.5090\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 0.6158 - acc: 0.6384 - val_loss: 0.7462 - val_acc: 0.5130\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 5s 510us/step - loss: 0.6102 - acc: 0.6379 - val_loss: 0.7448 - val_acc: 0.5130\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 5s 494us/step - loss: 0.6071 - acc: 0.6429 - val_loss: 0.7564 - val_acc: 0.5220\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 5s 489us/step - loss: 0.5965 - acc: 0.6523 - val_loss: 0.7661 - val_acc: 0.5190\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 5s 499us/step - loss: 0.5916 - acc: 0.6496 - val_loss: 0.7886 - val_acc: 0.5330\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 5s 510us/step - loss: 0.5816 - acc: 0.6685 - val_loss: 0.8055 - val_acc: 0.5180\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 0.5764 - acc: 0.6617 - val_loss: 0.8108 - val_acc: 0.5190\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 5s 485us/step - loss: 0.5700 - acc: 0.6718 - val_loss: 0.8303 - val_acc: 0.5040\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 5s 515us/step - loss: 0.5620 - acc: 0.6739 - val_loss: 0.8634 - val_acc: 0.5240\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 5s 502us/step - loss: 0.5587 - acc: 0.6777 - val_loss: 0.8412 - val_acc: 0.5200\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 5s 493us/step - loss: 0.5499 - acc: 0.6862 - val_loss: 0.8645 - val_acc: 0.5150\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 5s 483us/step - loss: 0.5380 - acc: 0.6938 - val_loss: 0.8846 - val_acc: 0.5210\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 5s 494us/step - loss: 0.5373 - acc: 0.6930 - val_loss: 0.9114 - val_acc: 0.5010\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 5s 496us/step - loss: 0.5294 - acc: 0.6950 - val_loss: 0.9182 - val_acc: 0.5140\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 0.5200 - acc: 0.7008 - val_loss: 0.9233 - val_acc: 0.5230\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 0.5152 - acc: 0.7114 - val_loss: 0.9399 - val_acc: 0.5130\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 5s 498us/step - loss: 0.5079 - acc: 0.7129 - val_loss: 0.9269 - val_acc: 0.5190\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 5s 485us/step - loss: 0.5029 - acc: 0.7170 - val_loss: 1.0160 - val_acc: 0.5240\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 0.4994 - acc: 0.7227 - val_loss: 1.0029 - val_acc: 0.5080\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 0.4866 - acc: 0.7284 - val_loss: 1.0307 - val_acc: 0.5110\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 5s 497us/step - loss: 0.4883 - acc: 0.7279 - val_loss: 1.0632 - val_acc: 0.5010\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 5s 481us/step - loss: 0.4802 - acc: 0.7367 - val_loss: 1.0748 - val_acc: 0.5170\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 0.4709 - acc: 0.7441 - val_loss: 1.0833 - val_acc: 0.5120\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 5s 513us/step - loss: 0.4675 - acc: 0.7407 - val_loss: 1.1535 - val_acc: 0.5140\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 5s 499us/step - loss: 0.4611 - acc: 0.7435 - val_loss: 1.1621 - val_acc: 0.5100\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 0.4525 - acc: 0.7508 - val_loss: 1.2098 - val_acc: 0.5110\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 5s 488us/step - loss: 0.4461 - acc: 0.7537 - val_loss: 1.1921 - val_acc: 0.5180\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 5s 490us/step - loss: 0.4470 - acc: 0.7514 - val_loss: 1.1695 - val_acc: 0.5100\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 5s 513us/step - loss: 0.4431 - acc: 0.7549 - val_loss: 1.2845 - val_acc: 0.5190\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 5s 498us/step - loss: 0.4388 - acc: 0.7618 - val_loss: 1.2249 - val_acc: 0.5020\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 0.4302 - acc: 0.7627 - val_loss: 1.3616 - val_acc: 0.5030\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 0.4263 - acc: 0.7692 - val_loss: 1.3015 - val_acc: 0.5020\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 5s 502us/step - loss: 0.4202 - acc: 0.7698 - val_loss: 1.3660 - val_acc: 0.5070\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 5s 483us/step - loss: 0.4135 - acc: 0.7702 - val_loss: 1.3589 - val_acc: 0.5200\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 5s 500us/step - loss: 0.4114 - acc: 0.7738 - val_loss: 1.3724 - val_acc: 0.5190\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 0.4079 - acc: 0.7792 - val_loss: 1.4443 - val_acc: 0.4920\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 5s 493us/step - loss: 0.4044 - acc: 0.7779 - val_loss: 1.4860 - val_acc: 0.5070\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 5s 468us/step - loss: 0.3997 - acc: 0.7804 - val_loss: 1.5696 - val_acc: 0.5000\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 0.3913 - acc: 0.7813 - val_loss: 1.5240 - val_acc: 0.4980\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 0.3963 - acc: 0.7824 - val_loss: 1.5234 - val_acc: 0.5070\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 5s 483us/step - loss: 0.3871 - acc: 0.7909 - val_loss: 1.6316 - val_acc: 0.5100\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 5s 485us/step - loss: 0.3876 - acc: 0.7911 - val_loss: 1.6353 - val_acc: 0.5120\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 5s 478us/step - loss: 0.3775 - acc: 0.7873 - val_loss: 1.6428 - val_acc: 0.5050\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 5s 494us/step - loss: 0.3775 - acc: 0.7879 - val_loss: 1.6361 - val_acc: 0.4940\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 5s 486us/step - loss: 0.3714 - acc: 0.7951 - val_loss: 1.6284 - val_acc: 0.5240\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 0.3706 - acc: 0.8000 - val_loss: 1.6555 - val_acc: 0.5030\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 0.3708 - acc: 0.7969 - val_loss: 1.7664 - val_acc: 0.5000\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 5s 491us/step - loss: 0.3626 - acc: 0.8028 - val_loss: 1.7356 - val_acc: 0.4950\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 5s 503us/step - loss: 0.3603 - acc: 0.7993 - val_loss: 1.8032 - val_acc: 0.5120\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 0.3518 - acc: 0.8063 - val_loss: 1.8370 - val_acc: 0.5040\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 5s 499us/step - loss: 0.3480 - acc: 0.8074 - val_loss: 1.9034 - val_acc: 0.5110\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 0.3521 - acc: 0.8070 - val_loss: 1.9359 - val_acc: 0.5040\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 5s 499us/step - loss: 0.3420 - acc: 0.8114 - val_loss: 1.9906 - val_acc: 0.5020\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 5s 483us/step - loss: 0.3430 - acc: 0.8116 - val_loss: 1.9266 - val_acc: 0.5040\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 0.3372 - acc: 0.8106 - val_loss: 1.9452 - val_acc: 0.4990\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 5s 497us/step - loss: 0.3290 - acc: 0.8156 - val_loss: 2.0072 - val_acc: 0.5180\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 0.3297 - acc: 0.8194 - val_loss: 1.9707 - val_acc: 0.5090\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 0.3364 - acc: 0.8126 - val_loss: 2.1172 - val_acc: 0.5110\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 0.3269 - acc: 0.8180 - val_loss: 2.1553 - val_acc: 0.5040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3CRWdPHa5uj",
        "colab_type": "text"
      },
      "source": [
        "### Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnL6K44la5uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'babi_chatbot_100_epochs.h5'\n",
        "model.save(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgakiyWWa5up",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "### Plotting Out Training History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZSqC83Ca5uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Plot out training history here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj239SG3a5uv",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on Given Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG8ZLcDfa5uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(filename)\n",
        "\n",
        "# TODO: Predict with the model\n",
        "pred_results = model.predict(([inputs_test,queries_test]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRvzvWb2a5uz",
        "colab_type": "code",
        "outputId": "e514b10f-c9d9-4986-a3fe-d27f42b6f349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "test_data[0][0]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mary',\n",
              " 'got',\n",
              " 'the',\n",
              " 'milk',\n",
              " 'there',\n",
              " '.',\n",
              " 'John',\n",
              " 'moved',\n",
              " 'to',\n",
              " 'the',\n",
              " 'bedroom',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LgMpkBja5u3",
        "colab_type": "code",
        "outputId": "558751ec-53d9-4097-fcbd-fd69d363728a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "story =' '.join(word for word in test_data[0][0])\n",
        "print(story)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mary got the milk there . John moved to the bedroom .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxCoLIzIa5u6",
        "colab_type": "code",
        "outputId": "583a2882-c884-491b-fd23-73c82b01d59a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "query = ' '.join(word for word in test_data[0][1])\n",
        "print(query)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is John in the kitchen ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTvBcWB7a5vD",
        "colab_type": "code",
        "outputId": "1494e790-2689-401e-8d49-c70b9fcca602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"True Test Answer from Data is:\",test_data[0][2])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Test Answer from Data is: no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0_qxIIma5vF",
        "colab_type": "code",
        "outputId": "1b9b553c-f76e-4689-e4eb-2f61524297d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Generate prediction from model\n",
        "val_max = np.argmax(pred_results[0])\n",
        "\n",
        "for key, val in tokenizer.word_index.items():\n",
        "    if val == val_max:\n",
        "        k = key\n",
        "\n",
        "print(\"Predicted answer is: \", k)\n",
        "print(\"Probability of certainty was: \", pred_results[0][val_max])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted answer is:  no\n",
            "Probability of certainty was:  0.50597763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyI57TGTa5vH",
        "colab_type": "text"
      },
      "source": [
        "## Writing Your Own Stories and Questions\n",
        "\n",
        "Remember you can only use words from the existing vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3oQ622ea5vH",
        "colab_type": "code",
        "outputId": "4a8d6131-fbec-4586-ffcf-adbfa5b2e9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " '?',\n",
              " 'Daniel',\n",
              " 'Is',\n",
              " 'John',\n",
              " 'Mary',\n",
              " 'Sandra',\n",
              " 'apple',\n",
              " 'back',\n",
              " 'bathroom',\n",
              " 'bedroom',\n",
              " 'discarded',\n",
              " 'down',\n",
              " 'dropped',\n",
              " 'football',\n",
              " 'garden',\n",
              " 'got',\n",
              " 'grabbed',\n",
              " 'hallway',\n",
              " 'in',\n",
              " 'journeyed',\n",
              " 'kitchen',\n",
              " 'left',\n",
              " 'milk',\n",
              " 'moved',\n",
              " 'no',\n",
              " 'office',\n",
              " 'picked',\n",
              " 'put',\n",
              " 'the',\n",
              " 'there',\n",
              " 'to',\n",
              " 'took',\n",
              " 'travelled',\n",
              " 'up',\n",
              " 'went',\n",
              " 'yes'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX0qU6cZa5vL",
        "colab_type": "code",
        "outputId": "a765e0e5-640f-4b63-f24a-8fcbdcd0570d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Note the whitespace of the periods\n",
        "my_story = \"John left the kitchen . Sandra dropped the football in the garden .\"\n",
        "my_story.split()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John',\n",
              " 'left',\n",
              " 'the',\n",
              " 'kitchen',\n",
              " '.',\n",
              " 'Sandra',\n",
              " 'dropped',\n",
              " 'the',\n",
              " 'football',\n",
              " 'in',\n",
              " 'the',\n",
              " 'garden',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Lnyvbma5vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_question = \"Is the football in the garden ?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AORXcLrta5vQ",
        "colab_type": "code",
        "outputId": "2f9632ad-c5c2-4ae5-c102-b2025cbd75d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "my_question.split()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Is', 'the', 'football', 'in', 'the', 'garden', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI7v0oHna5vS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata = [(my_story.split(),my_question.split(),'yes')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWuzBxsUa5vU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "4958beec-d06e-471a-a254-ae20865e8eda"
      },
      "source": [
        "my_story,my_ques,my_ans = vectorize_stories(mydata)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-8a07ff2895bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_story\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_ques\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_stories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-c5274ed0d263>\u001b[0m in \u001b[0;36mvectorize_stories\u001b[0;34m(data, word_index, max_story_len, max_question_len)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mxq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Index 0 Reserved when padding the sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-c5274ed0d263>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mxq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Index 0 Reserved when padding the sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m       \u001b[0;31m# Default: V1-style Graph execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterating over `tf.Tensor`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    521\u001b[0m     raise errors.OperatorNotAllowedInGraphError(\n\u001b[1;32m    522\u001b[0m         \u001b[0;34m\"{} is not allowed in Graph execution. Use Eager execution or decorate\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \" this function with @tf.function.\".format(task))\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BX98BWha5vX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "efa68924-b2fb-498d-afe6-b7eed3c0ddf1"
      },
      "source": [
        "pred_results = model.predict(([ my_story, my_ques]))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-4d458a430b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mmy_story\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_ques\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'my_ques' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erFlHtPba5va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate prediction from model\n",
        "val_max = np.argmax(pred_results[0])\n",
        "\n",
        "for key, val in tokenizer.word_index.items():\n",
        "    if val == val_max:\n",
        "        k = key\n",
        "\n",
        "print(\"Predicted answer is: \", k)\n",
        "print(\"Probability of certainty was: \", pred_results[0][val_max])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}